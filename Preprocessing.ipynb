{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjScHlxzafMzrvIdwCdnB2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SORYNICI/ML/blob/main/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKOPqa4w7Moo"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import codecs\n",
        "import pprint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "\n",
        "from torch import equal\n",
        "# You can import known libraries to write your own code\n",
        "import sys\n",
        "import io\n",
        "\n",
        "filePath = \"C://Download/HW01/\"\n",
        "\n",
        "def load_json(file_name):\n",
        "    \"\"\"\n",
        "    Load json file\n",
        "    :param file_name: file name\n",
        "    :return: loaded data from the file\n",
        "    \"\"\"\n",
        "    with codecs.open(file_name, 'r', encoding='utf-8') as json_f:\n",
        "        return json.load(json_f)\n",
        "\n",
        "\n",
        "def extract_data(dataset):\n",
        "    \"\"\"\n",
        "    Extract sentences and their labels from dataset\n",
        "    - human_sentence: list of the first human utterance in a conversation\n",
        "    - emotion_label: list of the emotion label.\n",
        "    The range of the emotion label is 0 to 5\n",
        "    0: 분노\n",
        "    1: 슬픔\n",
        "    2: 불안\n",
        "    3: 상처\n",
        "    4: 당황\n",
        "    5: 기쁨\n",
        "    :param dataset: loaded data from load_json function\n",
        "    :return: human_sentences and their emotion labels\n",
        "    \"\"\"\n",
        "\n",
        "    human_sentence = list()\n",
        "    emotion_label = list()\n",
        "\n",
        "    dialog_arr = {'HS01', 'HS02', 'HS03', 'SS01', 'SS02', 'SS03'}\n",
        "    for data in dataset:\n",
        "        for dialog in dialog_arr:\n",
        "            if not len(data['talk']['content'][dialog]) == 0:\n",
        "                human_sentence.append(data['talk']['content'][dialog])\n",
        "                # E68 에서 6 - 1\n",
        "                emotion_label.append(str(int(data['profile']['emotion']['type'][1]) -1))\n",
        "\n",
        "    # pass    # Task 1\n",
        "\n",
        "    return human_sentence, emotion_label\n",
        "\n",
        "\n",
        "def save_csv(file_name, sentences, labels):\n",
        "    \"\"\"\n",
        "    Save the sentences and labels to csv file\n",
        "    Header is needed to identify the column\n",
        "\n",
        "    :param file_name: output file name\n",
        "    :param sentences: human sentences from extract_data function\n",
        "    :param labels: emotion labels from extract_data function\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    # pass    # Task 2\n",
        "\n",
        "    idx = 0\n",
        "    with open(file_name, 'w', newline='\\n', encoding='utf-8-sig') as file:\n",
        "        write = csv.writer(file)\n",
        "        write.writerow([\"sentence\", \"label\"])\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            write.writerow([sentence, labels[i]])\n",
        "            idx += 1\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Entry of the program\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    training_data_json_filename = filePath + \"감성대화말뭉치(최종데이터)_Training.json\"\n",
        "    test_data_json_filename = filePath + \"감성대화말뭉치(최종데이터)_Validation.json\"\n",
        "\n",
        "    training_data_csv_filename = filePath + \"train.csv\"\n",
        "    valid_data_csv_filename = filePath + \"valid.csv\"\n",
        "    test_data_csv_filename = filePath + \"test.csv\"\n",
        "\n",
        "    test_size = 0.1\n",
        "    random_state = 108\n",
        "\n",
        "    # 1. Load json files\n",
        "    training_data = load_json(training_data_json_filename)\n",
        "    test_data = load_json(test_data_json_filename)\n",
        "    pprint.pprint(training_data[0])\n",
        "    print()\n",
        "\n",
        "    # 2. Extract data\n",
        "    training_sentences, training_labels = extract_data(training_data)\n",
        "    test_sentences, test_labels = extract_data(test_data)\n",
        "\n",
        "    # 3. Split training data to training and valid data\n",
        "    # Task 3\n",
        "    # Use train_test_split function\n",
        "    # Use test_size and random_state values to split the dataset\n",
        "\n",
        "    # test_size=0.2 20% 를 valid 에 사용\n",
        "    # shuffle\n",
        "    # stratify 한 쪽에 쏠려서 분배되는 것을 방지\n",
        "    # random_state 세트를 섞을 때 이 값을 참고, 데이터 셋이 매번 변경되는 것을 방지\n",
        "    training_sentences, valid_sentences, training_labels, valid_labels = train_test_split(training_sentences, training_labels, test_size=0.2, shuffle=True, stratify=training_labels, random_state=47)\n",
        "\n",
        "    print(\"# training data: \", len(training_sentences))\n",
        "    print(\"# validation data: \", len(valid_sentences))\n",
        "    print(\"# test data: \", len(test_sentences))\n",
        "\n",
        "    # 4. Save data to csv file\n",
        "    save_csv(training_data_csv_filename, training_sentences, training_labels)\n",
        "    save_csv(valid_data_csv_filename, valid_sentences, valid_labels)\n",
        "    save_csv(test_data_csv_filename, test_sentences, test_labels)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}